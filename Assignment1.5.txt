1. Hadoop in layman's term :
   

Hadoop can be thought of as a set of open source programs and procedures 
   (essentially they are free for anyone to use or modify, with a few exceptions) which anyone can use as the "backbone" of their big data operations.
   Hadoop is an open source, Java-based programming framework that supports the processing and storage of extremely large data sets in a distributed computing environment. It is part of the Apache project sponsored by the Apache Software Foundation









2. Components of Hadoop framework :
   Hadoop Distributed File System –Abbreviated as HDFS, it is primarily a file system similar to many of the already existing ones. However, it is also a virtual file system.
                                   There is one notable difference with other popular file systems, which is, when we move a file in HDFS, it is automatically split into smaller files. These smaller files are then replicated on a minimum of three different servers, 
                                   so that they can be used as an alternative to unforeseen circumstances.

   Hadoop MapReduce – MapReduce is mainly the programming aspect of Hadoop that allows processing of large volumes of data.
   HBASE – HBASE happens to be a layer that sits atop the HDFS and has been developed by means of the Java programming language. HBASE primarily has the following aspects –
           Non relational
           Highly scalable
           Fault tolerance
   Zookeeper – This is basically a centralized system that maintains –
               Configuration information
               Naming information
               Synchronization information
   Solr/Lucene – This is nothing but a search engine. Its libraries are developed by Apache and required over 10 years to be developed in its present robust form.
   Programming Languages – There are basically two programming languages that are identified as original Hadoop programming languages,
                             Hive
                             PIG 
   Besides these, there are a few other programming languages that can be used for writing programs, namely C, JAQL and Java. We can also make direct usage of SQL for interaction with the database, although that requires the use of standard JDBC or ODBC drivers.










3. Reasons to learn Big data technologies :
   Personal Reasons:
     1.New Job.
     2.High salary.
     3.High demand.
     4.Skill Development.
   
   Business Reason: A new style of IT emerging every 60 seconds,Oracle license cost increases every year.
   
   Others:
  Demand for Big Data skills is extremely high, and being able to prove your expertise is
critical:
  64% of IT hiring managers rate skilled big data knowledge as having extremely high or
high value when rating expertise of candidates, based on a survey by CompTIA
  According to Forbes, the median advertised salary for professionals with Big Data
expertise is $124,000 a year
   IBM, Cisco, and Oracle together advertised 26,488 open positions that required Big
Data expertise in the last twelve months
